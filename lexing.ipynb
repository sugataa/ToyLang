{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Lexical Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lexical analysis takes letters from a piece of source code and converts them into a stream of tokens which are values associated with a particular type. For example '3 + 4' might turn into the token (3, integer), ('', whitespace), (+, operator), (4, integer)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "simple_src = '4*6 + 9*(8-1)'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import namedtuple"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A token is defined by it's name, a pattern which is used to match it and (optional) callable which converts from the textual value of the token into a value. If this callable is None then the raw textual value of the token is used."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class TokenDef(namedtuple('TokenDef', ('name', 'pattern', 'value_filter'))):\n",
      "    def __repr__(self):\n",
      "        return 'TokenType.' + self.name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class TokenType(object):\n",
      "    _defs = [\n",
      "        # operators\n",
      "        TokenDef('plus', '+', None),\n",
      "        TokenDef('minus', '-', None),\n",
      "        TokenDef('asterisk', '*', None),\n",
      "        TokenDef('slash', '/', None),\n",
      "        \n",
      "        # other punctuation\n",
      "        TokenDef('left_paren', '(', None),\n",
      "        TokenDef('right_paren', ')', None),\n",
      "        \n",
      "        # more complex tokens\n",
      "        TokenDef('integer', re.compile('[0-9]+'), int),\n",
      "        TokenDef('whitespace', re.compile('[ \\t]+'), None),\n",
      "    ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for def_ in TokenType._defs:\n",
      "    setattr(TokenType, def_.name, def_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TokenType.integer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "TokenType.integer"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define a `Token` as a tuple of type (an attribute of 'TokenType'), a value and the slice of the input which it covers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Token = namedtuple('Token', ('type', 'value', 'slice'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define a function to return the first token from a piece of text."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def first_token(text, start=0):\n",
      "    \"\"\"Takes some text and an optional starting position within that string and\n",
      "    returns a Token representing the longest match at the front of that \n",
      "    string starting from *start*.\"\"\"\n",
      "    \n",
      "    match_text = text[start:]\n",
      "    \n",
      "    token = None\n",
      "    token_text = None\n",
      "    \n",
      "    for type_ in TokenType._defs:\n",
      "        name, pattern, value_filter = type_\n",
      "        if pattern is None:\n",
      "            continue\n",
      "        elif isinstance(pattern, str):\n",
      "            if not match_text.startswith(pattern):\n",
      "                continue\n",
      "            match_value = pattern\n",
      "        else:\n",
      "            match = pattern.match(match_text)\n",
      "            if not match:\n",
      "                continue\n",
      "            match_value = match.group(0)\n",
      "        \n",
      "        # see if the match value is longer than the current token's match value\n",
      "        if token_text is not None and len(token_text) >= len(match_value):\n",
      "            continue\n",
      "        \n",
      "        token_text = match_value\n",
      "        if value_filter is not None:\n",
      "            match_value = value_filter(match_value)\n",
      "        token = Token(type_, match_value, slice(start, start + len(token_text)))\n",
      "        \n",
      "    return token"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "first_token(' ')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "Token(type=TokenType.whitespace, value=' ', slice=slice(0, 1, None))"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "first_token('6')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "Token(type=TokenType.integer, value=6, slice=slice(0, 1, None))"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "first_token('68')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 42,
       "text": [
        "Token(type=TokenType.integer, value=68, slice=slice(0, 2, None))"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "first_token('68+')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 43,
       "text": [
        "Token(type=TokenType.integer, value=68, slice=slice(0, 2, None))"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "first_token('+')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 44,
       "text": [
        "Token(type=TokenType.plus, value='+', slice=slice(0, 1, None))"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The raw lexer repeatedly calls `first_token` on the text until all the text has been consumed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lex_raw(text):\n",
      "    start = 0\n",
      "    while True:\n",
      "        if start >= len(text):\n",
      "            break\n",
      "        \n",
      "        token = first_token(text, start)\n",
      "        yield token\n",
      "        start = token.slice.stop"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check that `lex_raw` is giving correct output."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list(lex_raw('8+9'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 46,
       "text": [
        "[Token(type=TokenType.integer, value=8, slice=slice(0, 1, None)),\n",
        " Token(type=TokenType.plus, value='+', slice=slice(1, 2, None)),\n",
        " Token(type=TokenType.integer, value=9, slice=slice(2, 3, None))]"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list(lex_raw('8 +9'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 47,
       "text": [
        "[Token(type=TokenType.integer, value=8, slice=slice(0, 1, None)),\n",
        " Token(type=TokenType.whitespace, value=' ', slice=slice(1, 2, None)),\n",
        " Token(type=TokenType.plus, value='+', slice=slice(2, 3, None)),\n",
        " Token(type=TokenType.integer, value=9, slice=slice(3, 4, None))]"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list(lex_raw('8 + 7 / (6-8)'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 48,
       "text": [
        "[Token(type=TokenType.integer, value=8, slice=slice(0, 1, None)),\n",
        " Token(type=TokenType.whitespace, value=' ', slice=slice(1, 2, None)),\n",
        " Token(type=TokenType.plus, value='+', slice=slice(2, 3, None)),\n",
        " Token(type=TokenType.whitespace, value=' ', slice=slice(3, 4, None)),\n",
        " Token(type=TokenType.integer, value=7, slice=slice(4, 5, None)),\n",
        " Token(type=TokenType.whitespace, value=' ', slice=slice(5, 6, None)),\n",
        " Token(type=TokenType.slash, value='/', slice=slice(6, 7, None)),\n",
        " Token(type=TokenType.whitespace, value=' ', slice=slice(7, 8, None)),\n",
        " Token(type=TokenType.left_paren, value='(', slice=slice(8, 9, None)),\n",
        " Token(type=TokenType.integer, value=6, slice=slice(9, 10, None)),\n",
        " Token(type=TokenType.minus, value='-', slice=slice(10, 11, None)),\n",
        " Token(type=TokenType.integer, value=8, slice=slice(11, 12, None)),\n",
        " Token(type=TokenType.right_paren, value=')', slice=slice(12, 13, None))]"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define the actual function which should be exported from this module."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lex = lex_raw"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check that the lexer can lex some real source code."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "src = '5+6*(8-1)/2-5'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list(lex(src))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 51,
       "text": [
        "[Token(type=TokenType.integer, value=5, slice=slice(0, 1, None)),\n",
        " Token(type=TokenType.plus, value='+', slice=slice(1, 2, None)),\n",
        " Token(type=TokenType.integer, value=6, slice=slice(2, 3, None)),\n",
        " Token(type=TokenType.asterisk, value='*', slice=slice(3, 4, None)),\n",
        " Token(type=TokenType.left_paren, value='(', slice=slice(4, 5, None)),\n",
        " Token(type=TokenType.integer, value=8, slice=slice(5, 6, None)),\n",
        " Token(type=TokenType.minus, value='-', slice=slice(6, 7, None)),\n",
        " Token(type=TokenType.integer, value=1, slice=slice(7, 8, None)),\n",
        " Token(type=TokenType.right_paren, value=')', slice=slice(8, 9, None)),\n",
        " Token(type=TokenType.slash, value='/', slice=slice(9, 10, None)),\n",
        " Token(type=TokenType.integer, value=2, slice=slice(10, 11, None)),\n",
        " Token(type=TokenType.minus, value='-', slice=slice(11, 12, None)),\n",
        " Token(type=TokenType.integer, value=5, slice=slice(12, 13, None))]"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Skipping Whitespace"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lex_skip_whitespace(text):\n",
      "    for token in lex_raw(text):\n",
      "        if token.type is TokenType.whitespace:\n",
      "            continue\n",
      "        yield token"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lex = lex_skip_whitespace"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list(lex('3 + 7'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "[Token(type=TokenType.integer, value=3, slice=slice(0, 1, None)),\n",
        " Token(type=TokenType.plus, value='+', slice=slice(2, 3, None)),\n",
        " Token(type=TokenType.integer, value=7, slice=slice(4, 5, None))]"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    }
   ],
   "metadata": {}
  }
 ]
}